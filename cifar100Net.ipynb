{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98ee40",
   "metadata": {},
   "source": [
    "## CIFAR 100 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79f88b",
   "metadata": {},
   "source": [
    "# Stage 1: Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0012c6",
   "metadata": {},
   "source": [
    "In this case we are going to develop a cifar 100 classification problem, for that we a re going to use the cifar100 dataset availablwe in torchvision, Although we are going to apply data augmentation for make the neuronal network more robust, and batch size of 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79018184",
   "metadata": {},
   "source": [
    "For this time i will use 256x256 size images, letting to the neuronal network learn better, so i have made some changes in stage 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12eb5f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch,os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, ConcatDataset, DataLoader\n",
    "\n",
    "#Augmentation data for being morerobust\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.ToTensor(),  # Converts the image into a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),  # Resizes the image to 64x64\n",
    "    transforms.ToTensor(),  # Converts the image into a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizes the tensors (mean and std deviation for 3 color channels)\n",
    "])\n",
    "\n",
    "# Create Train dataset\n",
    "train_dataset = datasets.CIFAR100(root = './dataset/train',download=True, train=True, transform=transform_train)\n",
    "# Create Test dataset\n",
    "test_dataset = datasets.CIFAR100(root = './dataset/test',download=True, train=False, transform=transform_test) \n",
    "\n",
    "#Create train loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#Create test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34ae2f",
   "metadata": {},
   "source": [
    "# Stage 2: building neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8a8b6",
   "metadata": {},
   "source": [
    "Stride: The stride refers to how many pixels the filter moves through the image or input volume at each step during the convolution operation. A stride of 1 means that the filter moves one pixel at a time. A stride of 2 means that the filter moves two pixels at a time, and so on. A larger stride will result in a lower spatial dimension output.\n",
    "\n",
    "Padding: Padding refers to the addition of extra pixels around the input image or volume before applying the convolution operation. The purpose of padding is to control the spatial dimension of the output. It is especially useful when you want to keep the spatial dimensions of the input and output the same after the convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8baf2e",
   "metadata": {},
   "source": [
    "For this time i will use 4 convolutional layer, letting to the network learn more complex forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea9e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # First convolutional layer: input channels = 3 (RGB), output channels = 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Second convolutional layer: input channels = 32 (from previous layer), output channels = 64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Third convolutional layer: input channels = 64 (from previous layer), output channels = 128\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # Fourth convolutional layer: input channels = 128 (from previous layer), output channels = 256\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # First fully connected layer, input size should match the output size of the last conv layer\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 500)\n",
    "        # Second fully connected layer, output size is the same as the number of classes\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply second conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Apply third conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # Apply fourth conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # Flatten the tensor output from the conv layers\n",
    "        x = x.view(-1, 256 * 16 * 16)\n",
    "        # Apply first fully connected layer with ReLU after applying dropout\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        # Apply second fully connected layer after applying dropout\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ab1b1",
   "metadata": {},
   "source": [
    "# Stage 3: Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d136e24",
   "metadata": {},
   "source": [
    "For this, we need to define a loss function and an optimiser. We will use Cross Entropy as our loss function, as it is a good choice for classification problems. For the optimiser, we will use Adam.\n",
    "\n",
    "Furthermore, we will divide our dataset into a training set and a validation set. During each epoch, we will train the model on the training set and then evaluate it on the validation set. If the performance on the validation set improves, we will save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ef7c0",
   "metadata": {},
   "source": [
    "At the beginning i used lr = 0.01 and dropdown 0.5, but thesystem couldnt learn, with the the actual system, using lr= 0.001 and dropdown = 0.2 and 42 epochs, improving for 5.7... to 2.622346130905637 and still getting better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e16073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is using: cuda\n",
      "Previous mode was loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 2.457882341305921\n",
      "El loss actual es 385.8875275850296 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 782/782 [02:57<00:00,  4.40it/s, training_loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 2.471396891934097\n",
      "El loss actual es 388.00931203365326 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=3.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Loss: 2.4793459670558855\n",
      "El loss actual es 389.25731682777405 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 2.47534755992282\n",
      "El loss actual es 388.6295669078827 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation Loss: 2.4352493232982173\n",
      "El loss actual es 382.33414375782013 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=3.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Validation Loss: 2.4388904624683843\n",
      "El loss actual es 382.9058026075363 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Validation Loss: 2.4659176981373196\n",
      "El loss actual es 387.1490786075592 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Validation Loss: 2.432062246237591\n",
      "El loss actual es 381.83377265930176 y el mejor es 381.35527324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 782/782 [02:57<00:00,  4.42it/s, training_loss=2.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Validation Loss: 2.4212390480527453\n",
      "El loss actual es 380.134530544281 y el mejor es 381.35527324676514\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Validation Loss: 2.4139975931993716\n",
      "El loss actual es 378.99762213230133 y el mejor es 380.134530544281\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Validation Loss: 2.418883190033542\n",
      "El loss actual es 379.7646608352661 y el mejor es 378.99762213230133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=3.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Validation Loss: 2.413293953913792\n",
      "El loss actual es 378.88715076446533 y el mejor es 378.99762213230133\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Validation Loss: 2.4218598308077284\n",
      "El loss actual es 380.23199343681335 y el mejor es 378.88715076446533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Validation Loss: 2.4128958427222673\n",
      "El loss actual es 378.82464730739594 y el mejor es 378.88715076446533\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Validation Loss: 2.3972633742982414\n",
      "El loss actual es 376.3703497648239 y el mejor es 378.82464730739594\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Validation Loss: 2.384923676016984\n",
      "El loss actual es 374.43301713466644 y el mejor es 376.3703497648239\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Validation Loss: 2.4300110362897254\n",
      "El loss actual es 381.5117326974869 y el mejor es 374.43301713466644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Validation Loss: 2.4310199842331515\n",
      "El loss actual es 381.6701375246048 y el mejor es 374.43301713466644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Validation Loss: 2.4399983040086783\n",
      "El loss actual es 383.0797337293625 y el mejor es 374.43301713466644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 782/782 [02:57<00:00,  4.42it/s, training_loss=3.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Validation Loss: 2.3580068782636316\n",
      "El loss actual es 370.20707988739014 y el mejor es 374.43301713466644\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Validation Loss: 2.4301258135753074\n",
      "El loss actual es 381.52975273132324 y el mejor es 370.20707988739014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 782/782 [02:56<00:00,  4.43it/s, training_loss=2.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Validation Loss: 2.3499532801330467\n",
      "El loss actual es 368.94266498088837 y el mejor es 370.20707988739014\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 782/782 [02:56<00:00,  4.43it/s, training_loss=3.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Validation Loss: 2.3892290349219256\n",
      "El loss actual es 375.1089584827423 y el mejor es 368.94266498088837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Validation Loss: 2.349580375252256\n",
      "El loss actual es 368.8841189146042 y el mejor es 368.94266498088837\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 782/782 [02:57<00:00,  4.42it/s, training_loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Validation Loss: 2.4026530312884384\n",
      "El loss actual es 377.21652591228485 y el mejor es 368.8841189146042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 782/782 [02:56<00:00,  4.43it/s, training_loss=3.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Validation Loss: 2.3621443175965813\n",
      "El loss actual es 370.85665786266327 y el mejor es 368.8841189146042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 782/782 [02:57<00:00,  4.41it/s, training_loss=2.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Validation Loss: 2.3597951087222735\n",
      "El loss actual es 370.487832069397 y el mejor es 368.8841189146042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 782/782 [02:56<00:00,  4.42it/s, training_loss=3.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Validation Loss: 2.3563966136069814\n",
      "El loss actual es 369.9542683362961 y el mejor es 368.8841189146042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 782/782 [02:56<00:00,  4.43it/s, training_loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Validation Loss: 2.376310438107533\n",
      "El loss actual es 373.0807387828827 y el mejor es 368.8841189146042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 782/782 [02:56<00:00,  4.43it/s, training_loss=2.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Validation Loss: 2.3991162754168176\n",
      "El loss actual es 376.66125524044037 y el mejor es 368.8841189146042\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Try to use cuda if posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"It is using: \" + device.type)\n",
    "\n",
    "# Initialice the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Path to save the model\n",
    "model_path = 'best_model.pth'\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Previous mode was loaded.\")\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "else:\n",
    "    print(\"Not previous model found.\")\n",
    "    model = Net().to(device)\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# Define a number of training epochs\n",
    "epochs = 30\n",
    "\n",
    "#actually is my best\n",
    "best_loss = 50\n",
    "\n",
    "best_val_loss = 368.8841189146042  # Initialize with a high value\n",
    "\n",
    "# Training loop\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming that we have 100 classes for the CIFAR-100 dataset\n",
    "class_names = [f'class_{i}' for i in range(100)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        # Move data to the GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix({'training_loss': loss.item()})\n",
    "\n",
    "    # Initialize lists to store predictions and labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move data to the GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update the validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate and print accuracy, recall, and F1-score\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Calculate and print confusion matrix\n",
    "     cm = confusion_matrix(all_labels, all_preds)\n",
    "     plt.figure(figsize=(12, 10))\n",
    "     sns.heatmap(cm, annot=True, fmt=\"d\", \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "     plt.ylabel(\"Real value\")\n",
    "     plt.xlabel(\"Predicted value\")\n",
    "     plt.show()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(test_loader)}')\n",
    "\n",
    "    # Save the model if it has the best validation loss so far\n",
    "    print(f'El loss actual es {val_loss} y el mejor es {best_val_loss}')\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"model saved\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d483f",
   "metadata": {},
   "source": [
    "# Stage 4 my own tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31550b95",
   "metadata": {},
   "source": [
    "As you can see below, the neuronal network still neading more epochs for improve its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1efe074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is using: cuda\n",
      "Predicted: ['oak_tree', 'shrew', 'otter', 'bee', 'rose', 'snail', 'keyboard', 'clock', 'table', 'squirrel', 'skyscraper', 'cockroach', 'sea', 'lobster', 'table', 'sunflower', 'ray', 'spider', 'train', 'chimpanzee', 'flatfish', 'dinosaur', 'bear', 'willow_tree', 'house', 'butterfly', 'chimpanzee', 'forest', 'plate', 'wardrobe', 'rose', 'caterpillar', 'wolf', 'cockroach', 'whale', 'orchid', 'leopard', 'baby', 'butterfly', 'crocodile', 'raccoon', 'shrew', 'beaver', 'oak_tree', 'porcupine', 'leopard', 'elephant', 'palm_tree', 'clock', 'table', 'pine_tree', 'baby', 'bed', 'train', 'raccoon', 'cup', 'chimpanzee', 'bed', 'bee', 'beetle', 'leopard', 'cockroach', 'snail', 'spider']\n",
      "True:      ['maple_tree', 'shrew', 'shark', 'bee', 'tulip', 'mushroom', 'telephone', 'hamster', 'sea', 'bicycle', 'skyscraper', 'butterfly', 'sea', 'leopard', 'table', 'sunflower', 'ray', 'spider', 'couch', 'cup', 'flatfish', 'dinosaur', 'mouse', 'willow_tree', 'house', 'pear', 'chimpanzee', 'television', 'hamster', 'wardrobe', 'sweet_pepper', 'lamp', 'wolf', 'cockroach', 'beaver', 'girl', 'leopard', 'table', 'lion', 'keyboard', 'raccoon', 'shrew', 'cattle', 'maple_tree', 'flatfish', 'tractor', 'wolf', 'skyscraper', 'clock', 'boy', 'pine_tree', 'bottle', 'bed', 'motorcycle', 'lion', 'telephone', 'dinosaur', 'bed', 'shrew', 'beetle', 'leopard', 'cockroach', 'tiger', 'bee']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"It is using: \" + device.type)\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of validation data\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# The outputs are probabilities for each class. To get the predicted class, we take the index of the highest probability.\n",
    "_, preds = torch.max(probabilities, 1)\n",
    "\n",
    "# Cargando las etiquetas de CIFAR-100\n",
    "with open('./dataset/train/cifar-100-python/meta', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='bytes')\n",
    "    fine_label_names = [t.decode('utf8') for t in data[b'fine_label_names']]\n",
    "\n",
    "# Utilizando las etiquetas para imprimir las clases predichas\n",
    "print('Predicted:', [fine_label_names[i] for i in preds])\n",
    "print('True:     ', [fine_label_names[i] for i in labels])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
