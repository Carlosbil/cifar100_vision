{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98ee40",
   "metadata": {},
   "source": [
    "## CIFAR 100 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79f88b",
   "metadata": {},
   "source": [
    "# Stage 1: Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0012c6",
   "metadata": {},
   "source": [
    "In this case we are going to develop a cifar 100 classification problem, for that we a re going to use the cifar100 dataset availablwe in torchvision, Although we are going to apply data augmentation for make the neuronal network more robust, and batch size of 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79018184",
   "metadata": {},
   "source": [
    "For this time i will use 256x256 size images, letting to the neuronal network learn better, so i have made some changes in stage 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12eb5f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch,os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, ConcatDataset, DataLoader\n",
    "\n",
    "#Augmentation data for being morerobust\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.ToTensor(),  # Converts the image into a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),  # Resizes the image to 64x64\n",
    "    transforms.ToTensor(),  # Converts the image into a tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizes the tensors (mean and std deviation for 3 color channels)\n",
    "])\n",
    "\n",
    "# Create Train dataset\n",
    "train_dataset = datasets.CIFAR100(root = './dataset/train',download=True, train=True, transform=transform_train)\n",
    "# Create Test dataset\n",
    "test_dataset = datasets.CIFAR100(root = './dataset/test',download=True, train=False, transform=transform_test) \n",
    "\n",
    "#Create train loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#Create test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34ae2f",
   "metadata": {},
   "source": [
    "# Stage 2: building neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8a8b6",
   "metadata": {},
   "source": [
    "Stride: The stride refers to how many pixels the filter moves through the image or input volume at each step during the convolution operation. A stride of 1 means that the filter moves one pixel at a time. A stride of 2 means that the filter moves two pixels at a time, and so on. A larger stride will result in a lower spatial dimension output.\n",
    "\n",
    "Padding: Padding refers to the addition of extra pixels around the input image or volume before applying the convolution operation. The purpose of padding is to control the spatial dimension of the output. It is especially useful when you want to keep the spatial dimensions of the input and output the same after the convolution operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8baf2e",
   "metadata": {},
   "source": [
    "For this time i will use 4 convolutional layer, letting to the network learn more complex forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea9e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # First convolutional layer: input channels = 3 (RGB), output channels = 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Second convolutional layer: input channels = 32 (from previous layer), output channels = 64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Third convolutional layer: input channels = 64 (from previous layer), output channels = 128\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # Fourth convolutional layer: input channels = 128 (from previous layer), output channels = 256\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # First fully connected layer, input size should match the output size of the last conv layer\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 500)\n",
    "        # Second fully connected layer, output size is the same as the number of classes\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply second conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Apply third conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # Apply fourth conv layer, followed by ReLU, then max pooling\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # Flatten the tensor output from the conv layers\n",
    "        x = x.view(-1, 256 * 16 * 16)\n",
    "        # Apply first fully connected layer with ReLU after applying dropout\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        # Apply second fully connected layer after applying dropout\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ab1b1",
   "metadata": {},
   "source": [
    "# Stage 3: Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d136e24",
   "metadata": {},
   "source": [
    "For this, we need to define a loss function and an optimiser. We will use Cross Entropy as our loss function, as it is a good choice for classification problems. For the optimiser, we will use Adam.\n",
    "\n",
    "Furthermore, we will divide our dataset into a training set and a validation set. During each epoch, we will train the model on the training set and then evaluate it on the validation set. If the performance on the validation set improves, we will save the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ef7c0",
   "metadata": {},
   "source": [
    "Actually it has been trained with 42 epochs, improveing for 5.7... to 2.622346130905637 and still getting better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e16073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is using: cuda\n",
      "Previous mode was loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 782/782 [02:34<00:00,  5.08it/s, training_loss=2.71]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Try to use cuda if posible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"It is using: \" + device.type)\n",
    "\n",
    "# Initialice the network\n",
    "model = Net().to(device)\n",
    "\n",
    "# Path to save the model\n",
    "model_path = 'best_model.pth'\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Previous mode was loaded.\")\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "else:\n",
    "    print(\"Not previous model found.\")\n",
    "    model = Net().to(device)\n",
    "    \n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# Define a number of training epochs\n",
    "epochs = 1\n",
    "\n",
    "#actually is my best\n",
    "best_loss = 50\n",
    "\n",
    "best_val_loss = 411.70834255218506  # Initialize with a high value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        # Move data to the GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix({'training_loss': loss.item()})\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move data to the GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update the validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss/len(test_loader)}')\n",
    "\n",
    "    # Save the model if it has the best validation loss so far\n",
    "    print(f'El loss actual es {val_loss} y el mejor es {best_val_loss}')\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"model saved\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d483f",
   "metadata": {},
   "source": [
    "# Stage 4 my own tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31550b95",
   "metadata": {},
   "source": [
    "As you can see below, the neuronal network still neading more epochs for improve its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1efe074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is using: cuda\n",
      "Predicted: ['caterpillar', 'chimpanzee', 'snake', 'cockroach', 'motorcycle', 'train', 'dolphin', 'sweet_pepper', 'bowl', 'shark', 'hamster', 'television', 'bee', 'sweet_pepper', 'maple_tree', 'bicycle', 'spider', 'hamster', 'cockroach', 'forest', 'lawn_mower', 'palm_tree', 'baby', 'motorcycle', 'rose', 'apple', 'streetcar', 'wardrobe', 'porcupine', 'tiger', 'pear', 'house', 'forest', 'tiger', 'orchid', 'bicycle', 'pear', 'cup', 'television', 'sunflower', 'bicycle', 'tulip', 'bed', 'couch', 'boy', 'bottle', 'wardrobe', 'bee', 'cockroach', 'cup', 'chimpanzee', 'ray', 'shark', 'kangaroo', 'pine_tree', 'sunflower', 'trout', 'trout', 'crab', 'tractor', 'motorcycle', 'sweet_pepper', 'sunflower', 'bicycle']\n",
      "True:      ['caterpillar', 'shark', 'tank', 'cockroach', 'cattle', 'bus', 'wolf', 'beetle', 'bowl', 'worm', 'bed', 'telephone', 'bee', 'sweet_pepper', 'willow_tree', 'chimpanzee', 'spider', 'hamster', 'cockroach', 'forest', 'lawn_mower', 'beetle', 'girl', 'motorcycle', 'rose', 'lamp', 'porcupine', 'television', 'porcupine', 'bee', 'pear', 'bridge', 'willow_tree', 'house', 'orchid', 'lizard', 'pine_tree', 'lamp', 'television', 'sunflower', 'bowl', 'tulip', 'television', 'couch', 'flatfish', 'pear', 'keyboard', 'woman', 'cockroach', 'tiger', 'chimpanzee', 'ray', 'shark', 'crocodile', 'pine_tree', 'sunflower', 'plate', 'shark', 'crab', 'lion', 'seal', 'sweet_pepper', 'sunflower', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"It is using: \" + device.type)\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of validation data\n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# The outputs are probabilities for each class. To get the predicted class, we take the index of the highest probability.\n",
    "_, preds = torch.max(probabilities, 1)\n",
    "\n",
    "# Cargando las etiquetas de CIFAR-100\n",
    "with open('./dataset/train/cifar-100-python/meta', 'rb') as file:\n",
    "    data = pickle.load(file, encoding='bytes')\n",
    "    fine_label_names = [t.decode('utf8') for t in data[b'fine_label_names']]\n",
    "\n",
    "# Utilizando las etiquetas para imprimir las clases predichas\n",
    "print('Predicted:', [fine_label_names[i] for i in preds])\n",
    "print('True:     ', [fine_label_names[i] for i in labels])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
